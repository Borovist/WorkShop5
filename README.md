# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]

Отчет по лабораторной работе #5 выполнил(а):
- Худяков Кирилл Александрович
- РИ-230942

Отметка о выполнении заданий (заполняется студентом):
| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.

## Цель работы: познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1 
### Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

В скрипте RollerAgent.cs или Move.cs коэффициент корреляции может влиять на обучение модели, определяя, насколько сильно вознаграждение зависит от действий агента. Например:
- Если коэффициент высокий, агент быстрее обучается, так как четко видит связь между действиями и вознаграждением.
- Если коэффициент низкий, обучение замедляется, так как агент не сразу понимает, какие действия приводят к успеху.

Вывод:
Коэффициент корреляции влияет на скорость и стабильность обучения. Оптимальное значение помогает избежать слишком быстрого (но нестабильного) или слишком медленного обучения.

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

В файле Economic.yaml можно изменить следующие параметры:
1. trainer_type (PPO и SAC):
   
- PPO (Proximal Policy Optimization) подходит для большинства задач.
- SAC (Soft Actor-Critic) лучше для задач с непрерывными действиями.

2. summary_freq (частота сохранения статистики):

- Увеличение значения уменьшает детализацию графиков в TensorBoard, но ускоряет обучение.
- Уменьшение значения дает более подробные данные, но замедляет процесс.

3. batch_size (разметка пакета для обучения):

- Большой batch_size ускоряет обучение, но требует больше памяти.
- Маленький batch_size замедляет обучение, но делает его более стабильным.

Вывод:
Изменение этих параметров позволяет оптимизировать процесс обучения под конкретную задачу.


## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Примеры использования ML-Agent'ов в игровых задачах:
1. Поиск объекта сферой

- Может использоваться в играх, где NPC нужно находить предметы или цели (например, сбор ресурсов, поиск пути в лабиринте).
- Проще использовать ML-агент, чем писать алгоритмы поиска, если поведение NPC должно адаптироваться к изменяющимся условиям.

2. Симулятор добычи ресурсов

- Подходит для экономических стратегий, где агенты должны оптимизировать добычу и транспортировку ресурсов.
- ML-агент полезен, если нужно моделировать сложное поведение (например, баланс между риском и прибылью).

Когда проще использовать ML-агент:
- Когда поведение NPC должно обучаться на основе опыта.
- Когда трудно заранее прописать все возможные сценарии поведения.
- Когда нужно тестировать итеративные улучшения AI.


## Вывод

ML-Agents помогает создавать умных игровых персонажей и виртуальных агентов, автоматически обучая их нужным действиям без ручного программирования.

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
